Skip to content
 
Search or jump toâ€¦

Pull requests
Issues
Marketplace
Explore
 
@pranjaljain99 
0
0 0 aditisaraswat10/Data-Analytics-
 Code  Issues 0  Pull requests 0  Projects 0  Wiki  Security  Insights
Data-Analytics-/code.py
@aditisaraswat10 aditisaraswat10 Rename Untitled9 (2).py to code.py
1b3236f on Apr 2
611 lines (358 sloc)  12.4 KB
    
#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from pandas import read_excel
import matplotlib as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import pylab as plot
import mglearn.plots
import seaborn as sns
from sklearn.manifold import Isomap
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn import svm
from mlxtend.plotting import plot_decision_regions
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_decision_regions
import tkinter as tk
from yellowbrick.classifier import ConfusionMatrix


# In[2]:


#Reading two files
df0=pd.read_excel(r"E:\Qmul\Data Analytics\ONLY_HDI.xlsx")
df0_=pd.read_excel(r"E:\Qmul\Data Analytics\All_other_datasets.xlsx")


# In[3]:


#Merging those files
df = pd.merge(df0,df0_,on=["Location_Name"])
df.to_excel("final.xlsx",index=False)


# In[4]:


df.head()


# In[5]:


df.dtypes


# In[6]:


df.shape


# In[7]:


#Checking for na values
df.isna().sum()


# In[8]:


#Drop na values
df1=df.dropna()
df1.isna().sum()


# In[9]:


df1.shape


# In[10]:


fig,ax=plot.subplots(1,1)
ax.scatter(df1["Location_Name"],df1["HDI_2017"])


# In[11]:


#Finding the status column
df1.loc[df1["HDI_2017"] <= 0.490, 'Status'] = 0
df1.loc[df1["HDI_2017"] >=0.750 , 'Status'] = 2
df1.loc[(df1["HDI_2017"] > 0.490) & (df1["HDI_2017"] < 0.750), 'Status'] = 1


# In[12]:


fig,ax=plot.subplots(1,1)
ax.scatter(df1["Status"],df1["HDI_2017"])


# In[13]:


#Count of status column
import seaborn as sns
sns.countplot(df1['Status'],label="Count")
plot.show()


# In[14]:


#HDI vs Status plot
var=df1.groupby(['Status']).sum().stack()
temp=var.unstack()
type(temp)
x_list = temp['HDI_2017']
label_list = temp.index
plot.axis("equal") 
plot.pie(x_list,labels=label_list,autopct="%1.1f%%")
plot.title("  HDI_2017 Status")
plot.legend(["Class0","Class1","Class2"])
plot.show()


# In[15]:


print(df1.groupby('Status').size())


# In[16]:


plot.figure(figsize=(8,5))
sns.distplot(df1["HDI_2017"])
plot.show()


# In[17]:


df_new=df1[["HDI_2017","export","import","Employment_share_in_pop","GDP_in_USD","GNI_in_USD","Loan_Share","Total_health_exp_in_share_of_GDP","Status"]]


# In[18]:


#Normalized data for denrose
normalized_df=(df_new-df_new.mean())/df_new.std()


# In[19]:


#Finding the correlation of each features
plot.figure(figsize=(15,8))
sns.heatmap(df_new.corr(),annot=True,cmap='coolwarm')
#sns.heatmap(normalized_df.corr(),annot=True,cmap='coolwarm')
plot.show()


# In[20]:


sns.pairplot(df_new, palette='rainbow')


# In[21]:


df1_new=df1[["HDI_2017","HDI_2016","HDI_2015","HDI_2014","HDI_2013","HDI_2012","HDI_2011","HDI_2010","HDI_2009","HDI_2008","HDI_2007"]]


# In[22]:


#Plot of HDI values
boxplot=df1_new.boxplot(return_type="axes")


# In[23]:


#DEnsity plot of HDI values
densityplot=df1_new.plot(kind="density")


# # Feature Selection

# In[24]:


#Check the Feature Importance
# importance score for each attribute where the larger score the more important the attribute
from sklearn.ensemble import ExtraTreesClassifier
data=df1.iloc[:,2:]
array = data.values
X = array[:,:18]
Y = array[:,-1]
# feature extraction
model = ExtraTreesClassifier()
model.fit(X, Y)
print(model.feature_importances_)


# In[25]:


#According to the score we are taking only HDI_2007 and HDI_2011
X=data.iloc[:,[0,4]].values
y=data.iloc[:,-1].astype(int) #target variable
y=y.values


# In[26]:


#Plot of HDI_2007 and HDI_2011 with three classes
mglearn.discrete_scatter(X[:,0],X[:,-1],y)
plot.xlabel("HDI_2007")
plot.ylabel("HDI_2011")
plot.legend(["Class 0","Class 1","Class 2"])


# In[27]:


#Splitting the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=.2)


# In[28]:


#Normalising the data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# # Logistic Regresssion

# In[29]:


from sklearn.linear_model import LogisticRegression
logistic = LogisticRegression()
logistic.fit(X_train, y_train)
print('Train Accuracy of Logistic regression classifier : {:.2f}'.format(logistic.score(X_train, y_train)))
print('Test Accuracy of Logistic regression classifier : {:.2f}'.format(logistic.score(X_test, y_test)))
y_pred=logistic.predict(X_test)
print("Mean squared error: %.2f" % np.mean(logistic.predict(X_test) - y_test) ** 2)


# In[30]:


from sklearn.manifold import Isomap
X_iso = Isomap(n_neighbors=10).fit_transform(X_train)
lgr = logistic.predict(X_train)
fig, ax = plot.subplots(1, 2, figsize=(8, 4))
fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)
ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=lgr,edgecolor='b')
ax[0].set_title('Predicted Training Labels')
ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train,edgecolor='b')
ax[1].set_title('Actual Training Labels')
plot.show()


# # KNN

# In[31]:


from sklearn.neighbors import KNeighborsClassifier
training_accuracy = []
test_accuracy = []
# try n_neighbors from 1 to 10
neighbours = range(1, 10)

for n in neighbours:
    knn = KNeighborsClassifier(n_neighbors=n)
    knn.fit(X_train, y_train)
    training_accuracy.append(knn.score(X_train, y_train))
    test_accuracy.append(knn.score(X_test, y_test))
    print('Accuracy of K-NN classifier : {:.2f}'.format(knn.score(X_test, y_test)))
plot.plot(neighbours,training_accuracy, label="training accuracy")
plot.plot(neighbours,test_accuracy, label="test accuracy")
plot.ylabel("Accuracy")
plot.xlabel("n_neighbors")
plot.legend()


# In[32]:


y_pred = knn.predict(X_test)
df_confusion = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
df_confusion


# In[33]:


ax= plot.subplot()
sns.heatmap((df_confusion / df_confusion.sum(axis=1)), annot=True, ax = ax); 
ax.set_xlabel('Predicted labels');ax.set_ylabel('Actual labels'); 
ax.set_title('Normalized Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Class 0', 'Class 1','Class 2']); ax.yaxis.set_ticklabels(['Class 0', 'Class 1','Class 2']);


# # Plot

# In[34]:


fig, axes = plot.subplots(1, 3, figsize = (17, 5))

for n_neighbors, ax in zip([1, 3, 30], axes):
    knn = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)
    mglearn.plots.plot_2d_separator(knn, X_train, fill=True, eps=0.5, ax=ax, alpha=.4)
    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)
    ax.set_title("{} neighbor(s)".format(n_neighbors))
    ax.set_xlabel("feature 0")
    ax.set_ylabel("feature 1")
    axes[0].legend(["Class0","Class 1","Class2"],loc=3)
    axes[1].legend(["Class0","Class 1","Class2"],loc=3)
    axes[2].legend(["Class0","Class 1","Class2"],loc=3)
    print('Train accuracy of K-NN classifier : {:.2f}'.format(knn.score(X_train, y_train)))
    print('Test accuracy of K-NN classifier : {:.2f}'.format(knn.score(X_test, y_test)))


# # SVM

# In[35]:


from sklearn.svm import LinearSVC

linear_svm = LinearSVC().fit(X_train, y_train)
mglearn.plots.plot_2d_classification(linear_svm,X_train)
mglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train)
line=np.linspace(-15,15)
for c,i,col in zip(linear_svm.coef_,linear_svm.intercept_,["b","r","g"]):
    plot.plot(line,-(line*c[0]+i)/c[1],c=col)
print('Train Accuracy of SVM classifier : {:.2f}'.format(linear_svm.score(X_train, y_train)))
print('Test Accuracy of SVM classifier: {:.2f}'.format(linear_svm.score(X_test, y_test)))
plot.xlabel("Feature 0")
plot.ylabel("Feature 1")
plot.legend(["Class 0","Class 1","Class 2","Class0 Line","Class1 Line","Class3 Line"])


# In[36]:


from yellowbrick.classifier import ClassificationReport
visualizer = ClassificationReport(linear_svm, classes=["Class0","Class1","Class2"])

visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer
visualizer.score(X_test, y_test)  # Evaluate the model on the test data
g = visualizer.poof()  


# # Decision Tree

# In[37]:


from sklearn.tree import DecisionTreeClassifier

decision = DecisionTreeClassifier(max_depth=3).fit(X_train,y_train)
print('Train Accuracy of Decision Tree classifier: {:.2f}'.format(decision.score(X_train, y_train)))
print('Test Accuracy of Decision Tree classifier: {:.2f}'.format(decision.score(X_test, y_test)))
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
h = .02  # step size in the mesh
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = decision.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plot.figure(1, figsize=(6, 5))
plot.pcolormesh(xx, yy, Z, cmap=plot.cm.Paired)

# Plot also the training points
plot.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k')
plot.xlabel('2007')
plot.ylabel('2011')

plot.xlim(xx.min(), xx.max())
plot.ylim(yy.min(), yy.max())
plot.xticks(())
plot.yticks(())

plot.show()


# In[38]:


cm = ConfusionMatrix(decision, classes=[0,1,2])
cm.fit(X_train, y_train)
cm.score(X_test, y_test)
cm.poof()


# In[39]:


#Prediction


# In[40]:


df1.drop(['Location_RegionId','export','import','Employment_share_in_pop','GDP_in_USD','GNI_in_USD','Loan_Share','Total_health_exp_in_share_of_GDP',"Status"], axis=1, inplace=True)

df1.head(10)


# In[41]:


#Giving the column names
df1.columns=["Location_Name","2007","2008","2009","2010","2011","2012","2013","2014","2015","2016","2017"]
#Taking transpose
df2 = df1.T
df2.head(10)


# In[42]:


#Creating new header
new_header = df2.iloc[0] #grab the first row for the header
df2 = df2[1:] #take the data less the header row
df2.columns = new_header
df2.head(10)


# In[43]:


df2.dtypes


# In[44]:


df3 = df2.apply(pd.to_numeric)
df3.dtypes


# In[45]:


#Plot of HDI-India
plot.plot(df3.index, df3['India'])
plot.title('India-HDI')
plot.ylabel('HDI');
plot.show()


# In[46]:


df3=df2.apply(pd.to_numeric)
df3


# In[47]:


y=df3.iloc[:,:]
y


# In[48]:


df3.index = pd.to_numeric(df3.index)
X = df3.index.values #features
y = df3[["India"]].values #target variable


# In[49]:


import statsmodels.api as sm # import statsmodels 
X = sm.add_constant(X) ## let's add an intercept (beta_0) to our model
X.shape


# In[50]:


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=.2)

#X_train.reshape(-1,1)
X_train.shape


# In[51]:


from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)


# In[52]:


df3.index


# In[53]:


confidence = lm.score(X_test, y_test)
print(confidence)


# In[54]:


forecast_set = lm.predict(X_test)
forecast_set


# In[55]:


Intercept_result =lm.intercept_
Intercept_result


# In[56]:


# tkinter GUI
import tkinter as tk
from tkinter import *
from tkinter.ttk import Combobox
root= tk.Tk()


# In[57]:


canvas1 = tk.Canvas(root, width = 600, height = 300)
canvas1.pack()
label1 = tk.Label(root, text=' Year:         ')
canvas1.create_window(120, 120, window=label1)
entry1 = tk.Entry (root) # create 1st entry box
canvas1.create_window(270, 120, window=entry1)


# In[58]:


def values(): 
    global HDI
    HDI = float(entry1.get())
    global Year #our 1st input variable
    Year = float(entry1.get()) 
    
    Prediction_result  = ('Predicted output: ', lm.predict([[HDI ,Year]]))
    label_Prediction = tk.Label(root, text= Prediction_result, bg='yellow',font=("Helvetica", 16))
    canvas1.create_window(280, 280, window=label_Prediction)
menu=Menu(root)
root.config(menu=menu)

subMenu=Menu(menu)
menu.add_cascade(label="Name of the Country", menu=subMenu)
subMenu.add_command(label="India",command=values)
subMenu.add_separator()
subMenu.add_command(label="Kyrgyzstan",command=values)
subMenu.add_separator()
subMenu.add_command(label="Kenya",command=values)
subMenu.add_separator()
subMenu.add_command(label="Ghana",command=values)
subMenu.add_separator()
subMenu.add_command(label="China",command=values)
subMenu.add_separator()
subMenu.add_command(label="United Kingdom",command=values)
subMenu.add_separator()
subMenu.add_command(label="Singapore",command=values)
subMenu.add_separator()
subMenu.add_command(label="Iraq",command=values)
subMenu.add_separator()
subMenu.add_command(label="United States",command=values)
subMenu.add_separator()
subMenu.add_command(label="Sudan",command=values)

root.mainloop()


